{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strojenie parametrów - czyli jak znaleźć najlepszy model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(123) \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r'''COLOR = 'white'\n",
    "matplotlib.rcParams['text.color'] = COLOR\n",
    "matplotlib.rcParams['axes.labelcolor'] = COLOR\n",
    "matplotlib.rcParams['xtick.color'] = COLOR\n",
    "matplotlib.rcParams['ytick.color'] = COLOR\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
    "matplotlib.rcParams['font.size'] = 14'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie danych i podział na train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('heart.csv')\n",
    "\n",
    "y = np.array(data['chd'])\n",
    "X = data.drop(['chd'],axis=1)\n",
    "\n",
    "map_dict = {'Present': 1, 'Absent':0}\n",
    "X['famhist'] = X['famhist'].map(map_dict)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drzewo decyzyjne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#parametry domyślne\n",
    "tree_model= DecisionTreeClassifier()\n",
    "\n",
    "tree_model.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Po co to wszystko?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](http://snoek.ddns.net/~oliver/mysite/images/blog7_accuracyprecision_banner.png)\n",
    "http://snoek.ddns.net/~oliver/mysite/the-bias-variance-tradeoff.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jak zdiagnozować problem? Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "tree_model= DecisionTreeClassifier(max_depth=5)\n",
    "train_sizes, train_scores, test_scores = \\\n",
    "    learning_curve(tree_model, X_train[:300], y_train[:300], train_sizes=np.linspace(0.1, 1, 100),\n",
    "                   scoring=\"accuracy\", cv=3)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.plot(train_sizes, test_scores.mean(1), 'o-', color=\"r\",\n",
    "         label=\"test\")\n",
    "plt.plot(train_sizes, train_scores.mean(1), 'o-', color=\"b\",\n",
    "         label=\"train\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Train size\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title('Learning curves')\n",
    "plt.legend(loc=\"best\")\n",
    "l = ax.legend()\n",
    "for text in l.get_texts():\n",
    "    text.set_color(\"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnoza: High Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "tree_model= DecisionTreeClassifier(max_depth=1)\n",
    "train_sizes, train_scores, test_scores = \\\n",
    "    learning_curve(tree_model, X_train[:300], y_train[:300], train_sizes=np.linspace(0.1, 1, 100),\n",
    "                   scoring=\"accuracy\", cv=3)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.plot(train_sizes, test_scores.mean(1), 'o-', color=\"r\",\n",
    "         label=\"test\")\n",
    "plt.plot(train_sizes, train_scores.mean(1), 'o-', color=\"b\",\n",
    "         label=\"train\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Train size\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title('Learning curves')\n",
    "plt.legend(loc=\"best\")\n",
    "l = ax.legend()\n",
    "for text in l.get_texts():\n",
    "    text.set_color(\"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powyżej mamy High Bias\n",
    "Andrew Ng: https://www.youtube.com/watch?v=ISBGFY-gBug&t=344s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kroswalidacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "tree_model= DecisionTreeClassifier()\n",
    "results=cross_val_score(tree_model, X, y) # można zdefiniować: scoring='roc_auc'\n",
    "# uwaga: tutaj nie strojono parametrów więc można użyć całego zbioru\n",
    "print(np.mean(results), np.std(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mamy wiele możliwych metryk\n",
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przeszukiwanie przestrzeni parametrów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "Klasyczne przeszukiwanie wszystkich możliwości\n",
    "\n",
    "#### Zalety:\n",
    "* sprawdzimy wszystkie kombinacje\n",
    "\n",
    "#### Wady:\n",
    "* musimy wiedzieć jakie konkretnie kombinacje chcemy sprawdzić\n",
    "* długo trwa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "max_depth=[3, 5, 6]\n",
    "criterion=[\"gini\",\"entrophy\"]\n",
    "ccp_alpha=[0, 0.05]\n",
    "param_grid = dict(max_depth=max_depth,criterion=criterion, ccp_alpha=ccp_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model= DecisionTreeClassifier()\n",
    "grid = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv = 3, n_jobs=-1)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train) #tutaj lepiej zastosować tylko trainset\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = RandomizedSearchCV(estimator=tree_model, param_distributions=param_grid, cv = 3, n_jobs=-1)\n",
    "\n",
    "random_result = random.fit(X_train, y_train)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=random_result.best_estimator_\n",
    "best_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważmy, że tu mało się zmienia. Nadal przeszukujemy siatkę, ale w sposób losowy. Nasza siatka jest dość mała, więc i tak sprawdzamy wszystkie możliwości"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zdefiniowanie rozkładów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://ksopyla.com/wp-content/uploads/2018/12/Grid_search_vs_random_search_cross_validation.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "sos: https://ksopyla.com/machine-learning/grid-random-search-scikit-learn-dobor-parametrow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson,expon\n",
    "param_grid = {'ccp_alpha': expon(0.08),\n",
    "             'max_depth': poisson(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = RandomizedSearchCV(estimator=tree_model, param_distributions=param_grid, cv = 3, n_jobs=-1, random_state=123)\n",
    "\n",
    "random_result = random.fit(X_train, y_train)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes optimization\n",
    "Bardziej inteligentne przeszukiwanie na podstawie Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-optimize\n",
    "#!pip install --upgrade scikit-learn==0.23.2\n",
    "#Jest problem z najnowszą wersją sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    SVC(),\n",
    "    {\n",
    "        'degree': (1, 8),  # integer valued parameter\n",
    "        'kernel': ['linear', 'poly', 'rbf'],  # categorical parameter\n",
    "    },\n",
    "    n_iter=8, # tu powinno być więcej, ale to się długo liczy\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "print(\"val. score: %s\" % opt.best_score_)\n",
    "print(\"test score: %s\" % opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykład pochodzi stąd: https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html  \n",
    "Tam też jest więcej parametrów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Są narzędzia, które robią wszystko za nas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## automl from sklearn\n",
    "(działa tylko na Linuxie - rozwiązanie: korzystać z Google Colaboratory)  \n",
    "Nie ma na Anacondzie, wymaga sklearn >= 0.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import autosklearn.classification\n",
    "automl = autosklearn.classification.AutoSklearnClassifier()\n",
    "automl.fit(X_train, y_train)\n",
    "y_pred = automl.predict(X_test)\n",
    "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać trudno się czasem dogadać ze wszystkimi pakietami..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT\n",
    "https://epistasislab.github.io/tpot/  \n",
    "Korzysta z algorytmów genetycznych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537396029/output_2_0_d7uh0v.png)\n",
    "https://www.datacamp.com/community/tutorials/tpot-machine-learning-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "#from tpot import TPOTRegressor\n",
    "\n",
    "tpot = TPOTClassifier(generations=5,verbosity=2)\n",
    "\n",
    "tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potencjalne wady:\n",
    "- długo się liczy\n",
    "- nie gwarantuje zbieżności\n",
    "\n",
    "Trochę więcej (np. o parametrach) można poczytać tutaj https://www.datacamp.com/community/tutorials/tpot-machine-learning-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selekcja zmiennych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtry\n",
    "Najprostsze metody, nie zależą od moeli. To jest preprocessing.\n",
    "\n",
    "Przykłady:\n",
    "- korelacja ze zmienną celu\n",
    "- informacja wzajema ze zmienną celu (VIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIF\n",
    "Variance Inflation Factor  \n",
    "Dopasowujemy model regresji liniowej, gdzie jedna kolumna jest targetem, a pozostałe feature'ami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "pd.DataFrame(data = [(X_train.columns[i], vif(X_train.values, i)) for i in range(X_train.shape[1])], columns=['feature', 'vif_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metody wbudowane\n",
    "czyli takie metody, które są wbudowane w algorytm - Lasso albo Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importance\")\n",
    "plt.bar(X.columns, importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrappery\n",
    "Oceniają poszczególne zbiory zmiennych na podstawie wybranych metryk (accuracy, AUC, ...). Zazwyczaj działa to w ten sposób, że iteracyjnie dodajemy/odejmujemy kolejne zmienne aż osiągniemy daną liczbę zmiennych/wynik modelu.   \n",
    "\n",
    "Zasadniczo to taki model, który ocenia zmienne. Też trzeba go zfitować.\n",
    "\n",
    "Przykłady:\n",
    "- Forward Selection (K Best, Select From Model) \n",
    "- Recursive Feature Elimination\n",
    "- Boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectKBest \n",
    "Wybieramy K Najlepszych cech na podstawie zadanego kryterium  \n",
    "Default: na podstawie algorytmu ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "tree_model=DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('select', SelectKBest()),\n",
    "    ('model', tree_model)])\n",
    "\n",
    "k=[5,6]\n",
    "max_depth=[3, 5, 6]\n",
    "criterion=[\"gini\",\"entrophy\"]\n",
    "\n",
    "# uwaga: gdy podajemy parametry do strojenia gdy mamy pipeline to trzeba w nazwach kluczy podać nazwę_danego_etapu__ \n",
    "# (poprzedzoną dwoma podkreślnikami)\n",
    "param_grid = {\"model__max_depth\": max_depth, \"model__criterion\": criterion, \"select__k\": k}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, cv=5).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns[search.best_estimator_.steps[0][1].get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select From Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_selector = SelectFromModel(\n",
    "    LogisticRegression(C=0.000025, solver=\"liblinear\"),\n",
    "    threshold = \"mean\"\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('select', model_selector),\n",
    "    ('model', tree_model)])\n",
    "\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# tu odwołujemy się do select (SelectFromModel) i do estymatora w środku (LR), więc dwa razy podwójny podkreślnik (__)\n",
    "param_grid = {\"model__max_depth\": max_depth, \"model__criterion\": criterion, \"select__estimator__penalty\": penalty}\n",
    "search = GridSearchCV(pipe, param_grid, cv=5).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns[search.best_estimator_.steps[0][1].get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination\n",
    "Zasadniczo, działa tak jak brzmi. \n",
    "Dopasuj model, zobacz, która cecha jest \"najmniej ważna\".  \n",
    "Powtórz dla modelu bez tej cechy.  \n",
    "\n",
    "Najmniej ważna to może być względem *feature_importances* albo *coef*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "estimator = tree_model\n",
    "selector = RFE(estimator, n_features_to_select=3, step=1) #step ile (procent) cech usuwamy w kroku\n",
    "selector = selector.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selector.support_)\n",
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boruta\n",
    "Próbuje znaleźć *wszystkie* cechy, które mają związek z targetem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "# for classification only (we need to convert pd.DataFrame to np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n",
    "\n",
    "feat_selector.fit(X_train.values, y_train)\n",
    "feat_selector.support_\n",
    "feat_selector.ranking_\n",
    "\n",
    "# wybór odpowiednich zmiennych ze zbioru testowego\n",
    "X_filtered = feat_selector.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns[feat_selector.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ciekawostki: \n",
    "Przykład wizualizacji dla kilku metryk \n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html   \n",
    "\n",
    "Jak robić strojenie parametrów dla różnych modeli?\n",
    "- http://www.davidsbatista.net/blog/2018/02/23/model_optimization/\n",
    "- https://stackoverflow.com/questions/50265993/alternate-different-models-in-pipeline-for-gridsearchcv  \n",
    "\n",
    "Inny przykład automl\n",
    "- Hyperopt https://hyperopt.github.io/hyperopt/\n",
    "\n",
    "Paulina wrzuciła ten link a propos metod wbudowanych, więc nie chciałem być gorszy\n",
    "- https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
