{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interesting-consensus",
   "metadata": {},
   "source": [
    "# Zadanie domowe 2\n",
    "### Mateusz Sperkowski\n",
    "W tym zadaniu korzystamy ze zbioru Allegro [https://www.dropbox.com/s/360xhh2d9lnaek3/allegro-api-transactions.csv?dl=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"allegro-api-transactions.csv\", index_col = \"lp\")\n",
    "print(data.columns)\n",
    "cell2=len(data[\"it_location\"].unique())\n",
    "print(f\"Unique number of it_locations: {cell2}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"it_location\"] = data[\"it_location\"].apply(str.lower)\n",
    "c = len(data[\"it_location\"].unique())\n",
    "print(f\"Unique number of it_locations after transformation: {c}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-event",
   "metadata": {},
   "source": [
    "Należy zauważyć że w oryginalnych danych wartości \"it_location\" które powinny być takie same, przykładowo były napisane raz stylem wielbłądzim a raz wszystkie wielkimi literami. Zastosowałem przekształcenie wszystkich wyrazów do tylko małych liter co ograniczyło powtórzenia o 2 tysiące różnych wartości (miast). Przykład: \"BIAŁYSTOK\" i \"Białystok\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-encounter",
   "metadata": {},
   "source": [
    "## Kodowanie zmiennych kategorycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.TargetEncoder()\n",
    "data['it_location'] = encoder.fit_transform(data['it_location'], data['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique number of it_locations after transformation: {len(data['it_location'].unique())}\")\n",
    "data[\"it_location\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-trail",
   "metadata": {},
   "source": [
    "Target Encoding nie zwiększa wymiarowości problemu, co jest tu ważne gdy mamy prawie 8k miast, one hot by dodał 8k kolumn. Trzymając w jednej kolumnie te wartości można wyliczyć odległość między wartościami, co może i nie byłoby tak złe gdy są to miasta, jednak encoding ten oczywiście nie ma sposobu na takie określenie tego by to miało sens. Wiec odległość miedzy dwoma wartościami będzie bezsensowna informacją.\n",
    "\n",
    "Target Encoding: Działa na bazie prawdopodobieństwa aposteriori (tzn. prawdopodobieństwo na podstawie doświadczenia losowego/zbioru testowego) i prawdopodobieństwa \"oczekiwanego\" dla zmiennej kategorycznej. Po kodowaniu tym zmniejszyła się ilość unikalnych wartości kolumny \"it_location\", co prawdopodobnie oznacza że przypisana została ta sama wartość dla różnych \"it_location\". Tak jakby kolumna kategoryczna została stransformowana do numerycznej. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[\"main_category\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-organizer",
   "metadata": {},
   "source": [
    "### Onehot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded_onehot = pd.DataFrame(onehot.fit_transform(data[[\"main_category\"]]).toarray())\n",
    "data_onehot = data.join(encoded_onehot)\n",
    "data_onehot.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-drive",
   "metadata": {},
   "source": [
    "Jest mało (27) unikalnych wartości w \"main_category\", co pozwala zapewnia że moglibyśmy spokojnie używać one hot encoding. Problemem byłoby gdyby występowało znacznie więcej różnych wartości."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-master",
   "metadata": {},
   "source": [
    "### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal = ce.ordinal.OrdinalEncoder()\n",
    "encoded_ordinal = pd.DataFrame(ordinal.fit_transform(data[[\"main_category\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(encoded_ordinal[\"main_category\"].unique()))\n",
    "encoded_ordinal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-installation",
   "metadata": {},
   "source": [
    "Losowo przypisane są wartości całkowite do unikalnych wartości w \"main_category\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-contemporary",
   "metadata": {},
   "source": [
    "### Count Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = ce.count.CountEncoder(normalize=True)\n",
    "encoded_count = pd.DataFrame(count.fit_transform(data[[\"main_category\"]]))\n",
    "encoded_count.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-peter",
   "metadata": {},
   "source": [
    "Zmiennej kategorycznej przypisana jest liczba jej częstości (tu dodatkowo znormalizowana do przedziału (0,1)). Więc w przypadku równych częstości różnych zmiennych, przypisane zostaną do tej samej wartości po encodingu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-ferry",
   "metadata": {},
   "source": [
    "## Uzupełnianie braków"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-sustainability",
   "metadata": {},
   "source": [
    " uzupełnić z użyciem jednego z automatycznych narzędzi: Nearest neighbors imputation lub Multivariate feature imputation (https://scikit-learn.org/stable/modules/impute.html).\n",
    " \n",
    "Opisać wnioski z analizy jakości imputacji i umieścić podsumowujący wykres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 210000\n",
    "#size = 21000\n",
    "random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0 = []\n",
    "print(\"Iteration: \", end=\"\")\n",
    "for i in range(10):\n",
    "    data2 = data[[\"price\", \"it_seller_rating\", \"it_quantity\"]]\n",
    "    data2 = data2.iloc[0:size, :]\n",
    "    ix = [row for row in range(data2.shape[0])]\n",
    "    for row in random.sample(ix, int(round(.1*len(ix)))):\n",
    "        data2.loc[row, \"it_seller_rating\"] = np.nan\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "    data2 = imputer.fit_transform(data2)\n",
    "    result = mean_squared_error(data.loc[0:(size-1),\"it_seller_rating\"], data2[:,1], squared=False)\n",
    "    res0.append([i,result])\n",
    "    print(i, end=\"\")\n",
    "print(\", Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [val for (i, val) in res0]\n",
    "print(f\"it_seller_rating std: {np.std(z)}\")\n",
    "avg_z = np.mean(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[0,1,2,3,4,5,6,7,8,9]\n",
    "sns.barplot(x=x, y=z, color=\"tab:blue\")\n",
    "sns.lineplot(x=x, y=[avg_z]*10, color=\"red\")\n",
    "plt.title(\"Plot Of Error in iterations and mean error line\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = []\n",
    "print(\"Iteration: \", end=\"\")\n",
    "for i in range(10):\n",
    "    data2 = data[[\"price\", \"it_seller_rating\", \"it_quantity\"]]\n",
    "    data2 = data2.iloc[0:size, :]\n",
    "    ix = [row for row in range(data2.shape[0])]\n",
    "    for row in random.sample(ix, int(round(.1*len(ix)))):\n",
    "        data2.loc[row, \"it_seller_rating\"] = np.nan\n",
    "    for row in random.sample(ix, int(round(.1*len(ix)))):\n",
    "        data2.loc[row, \"it_quantity\"] = np.nan\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "    data2 = imputer.fit_transform(data2)\n",
    "    result = mean_squared_error(data.loc[0:(size-1),\"it_seller_rating\"], data2[:,1], squared=False)\n",
    "    result2 = mean_squared_error(data.loc[0:(size-1),\"it_quantity\"], data2[:,2], squared=False)\n",
    "    res2.append([i,result, result2])\n",
    "    print(i, end=\"\")\n",
    "print(\", Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = []\n",
    "quant = []\n",
    "for (i, rating, quantity) in res2:\n",
    "    rate.append(rating)\n",
    "    quant.append(quantity)\n",
    "\n",
    "rate_avg = np.mean(rate)\n",
    "quant_avg = np.mean(quant)\n",
    "print(f\"it_seller_rating std: {np.std(rate)}\")\n",
    "print(f\"it_quantity std: {np.std(quant)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[0,1,2,3,4,5,6,7,8,9]\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize = (14,6))\n",
    "sns.barplot(x=x, y=rate, color=\"tab:blue\", ax = ax1)\n",
    "sns.lineplot(x=x, y=[rate_avg]*10, color=\"red\", ax= ax1)\n",
    "sns.barplot(x=x, y=quant, color=\"tab:blue\", ax = ax2)\n",
    "sns.lineplot(x=x, y=[quant_avg]*10, color=\"red\", ax= ax2)\n",
    "plt.suptitle(\"Plot Of Error in iterations and mean error line\")\n",
    "ax1.title.set_text('Plot for it_seller_rating')\n",
    "ax2.title.set_text('Plot for it_quantity')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-kelly",
   "metadata": {},
   "source": [
    "### Wnioski z imputacji\n",
    "\n",
    "Należy wziąć pod uwagę że RMSE liczone jest na całym zbiorze. Biorąc małe podzbiory danych błędy w kolejnych iteracjach bardzo się różniły, jednak używając większość zbioru były one prawie identyczne. Automatyczna imputacja jest szybkim rozwiązaniem, lecz wiążę się z dużym błędem oraz wysokim odchyleniem standardowym (duży rozrzut błędu). Im wiecej kolumn ma brakujące wartości, tym większe jest odchylenie standardowe tego błędu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WUM_python",
   "language": "python",
   "name": "wum_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
